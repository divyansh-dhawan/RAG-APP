{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dd9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai qdrant-client cohere streamlit python-dotenv langchain langchain-google-genai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bf0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import qdrant_client\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0b0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion and Chunking\n",
    "def process_and_chunk_data(text_data, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Processes and chunks the text data.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    docs = text_splitter.split_text(text_data)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadatas = [{\n",
    "        \"source\": f\"chunk-{i}\",\n",
    "        \"title\": \"User Uploaded Data\",\n",
    "        \"position\": i\n",
    "    } for i in range(len(docs))]\n",
    "    \n",
    "    return docs, metadatas\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# sample_text = \"Your long text document goes here...\"\n",
    "# documents, metadatas = process_and_chunk_data(sample_text)\n",
    "# print(f\"Number of chunks: {len(documents)}\")\n",
    "# print(documents[0])\n",
    "# print(metadatas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store (Qdrant)\n",
    "def get_qdrant_vector_store(collection_name=\"user-data-collection\"):\n",
    "    \"\"\"\n",
    "    Initializes and returns a Qdrant vector store.\n",
    "    \"\"\"\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        qdrant_url,\n",
    "        api_key=qdrant_api_key\n",
    "    )\n",
    "    \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    \n",
    "    vector_store = Qdrant(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def upsert_data_to_qdrant(documents, metadatas, collection_name=\"user-data-collection\"):\n",
    "    \"\"\"\n",
    "    Upserts documents and metadatas to the Qdrant collection.\n",
    "    \"\"\"\n",
    "    vector_store = get_qdrant_vector_store(collection_name)\n",
    "    vector_store.add_texts(documents, metadatas)\n",
    "    print(f\"Upserted {len(documents)} chunks to Qdrant collection '{collection_name}'.\")\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# upsert_data_to_qdrant(documents, metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5376b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever and Reranker\n",
    "def get_retriever(collection_name=\"user-data-collection\", top_k=5):\n",
    "    \"\"\"\n",
    "    Initializes and returns a retriever for the Qdrant vector store.\n",
    "    \"\"\"\n",
    "    vector_store = get_qdrant_vector_store(collection_name)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    return retriever\n",
    "\n",
    "def rerank_documents(query, documents, top_n=3):\n",
    "    \"\"\"\n",
    "    Reranks the retrieved documents using Cohere's rerank API.\n",
    "    \"\"\"\n",
    "    co = cohere.Client(cohere_api_key)\n",
    "    \n",
    "    docs = [doc.page_content for doc in documents]\n",
    "    results = co.rerank(query=query, documents=docs, top_n=top_n, model=\"rerank-english-v2.0\")\n",
    "    \n",
    "    reranked_docs = [documents[result.index] for result in results.results]\n",
    "    return reranked_docs\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# retriever = get_retriever()\n",
    "# retrieved_docs = retriever.get_relevant_documents(\"your query\")\n",
    "# reranked_docs = rerank_documents(\"your query\", retrieved_docs)\n",
    "# for i, doc in enumerate(reranked_docs):\n",
    "#     print(f\"Reranked Doc {i+1}:\")\n",
    "#     print(doc.page_content)\n",
    "#     print(doc.metadata)\n",
    "#     print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM and Answering\n",
    "def get_conversational_chain():\n",
    "    \"\"\"\n",
    "    Initializes and returns a conversational retrieval QA chain.\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "    \n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=get_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "def generate_response(query, chain):\n",
    "    \"\"\"\n",
    "    Generates a response to the user's query using the conversational chain.\n",
    "    \"\"\"\n",
    "    result = chain({\"query\": query})\n",
    "    \n",
    "    answer = result[\"result\"]\n",
    "    source_documents = result[\"source_documents\"]\n",
    "    \n",
    "    # Create citations\n",
    "    citations = []\n",
    "    for i, doc in enumerate(source_documents):\n",
    "        citation = f\"[{i+1}] {doc.metadata.get('title', 'Unknown Title')} (chunk {doc.metadata.get('position', 'N/A')})\"\n",
    "        citations.append(citation)\n",
    "        \n",
    "    # Add citations to the answer\n",
    "    full_response = f\"{answer}\\n\\n**Sources:**\\n\" + \"\\n\".join(citations)\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# chain = get_conversational_chain()\n",
    "# response = generate_response(\"your query\", chain)\n",
    "# print(response)"
   ]
  },
 "nbformat": 4,
 "nbformat_minor": 5
}
