{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dd9209",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai qdrant-client cohere streamlit python-dotenv langchain langchain-google-genai langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65bf0bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import qdrant_client\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de0b0245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion and Chunking\n",
    "def process_and_chunk_data(text_data, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Processes and chunks the text data.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    docs = text_splitter.split_text(text_data)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadatas = [{\n",
    "        \"source\": f\"chunk-{i}\",\n",
    "        \"title\": \"User Uploaded Data\",\n",
    "        \"position\": i\n",
    "    } for i in range(len(docs))]\n",
    "    \n",
    "    return docs, metadatas\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# sample_text = \"Your long text document goes here...\"\n",
    "# documents, metadatas = process_and_chunk_data(sample_text)\n",
    "# print(f\"Number of chunks: {len(documents)}\")\n",
    "# print(documents[0])\n",
    "# print(metadatas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fd155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store (Qdrant)\n",
    "def get_qdrant_vector_store(collection_name=\"user-data-collection\"):\n",
    "    \"\"\"\n",
    "    Initializes and returns a Qdrant vector store.\n",
    "    \"\"\"\n",
    "    client = qdrant_client.QdrantClient(\n",
    "        qdrant_url,\n",
    "        api_key=qdrant_api_key\n",
    "    )\n",
    "    \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "    # Check if collection exists, if not create it\n",
    "    try:\n",
    "        client.get_collection(collection_name=collection_name)\n",
    "    except Exception as e:\n",
    "        # If collection does not exist, create it\n",
    "        client.recreate_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config=qdrant_client.http.models.VectorParams(size=768, distance=qdrant_client.http.models.Distance.COSINE)\n",
    "        )\n",
    "        print(f\"Collection '{collection_name}' created.\")\n",
    "\n",
    "    vector_store = Qdrant(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "def upsert_data_to_qdrant(documents, metadatas, collection_name=\"user-data-collection\", batch_size=32):\n",
    "    \"\"\"\n",
    "    Upserts documents and metadatas to the Qdrant collection in batches.\n",
    "    \"\"\"\n",
    "    vector_store = get_qdrant_vector_store(collection_name)\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "        batch_metadatas = metadatas[i:i+batch_size]\n",
    "        vector_store.add_texts(batch_docs, batch_metadatas)\n",
    "    print(f\"Upserted {len(documents)} chunks to Qdrant collection '{collection_name}'.\")\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# upsert_data_to_qdrant(documents, metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf5376b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever and Reranker\n",
    "def get_retriever(collection_name=\"user-data-collection\", top_k=5):\n",
    "    \"\"\"\n",
    "    Initializes and returns a retriever for the Qdrant vector store.\n",
    "    \"\"\"\n",
    "    vector_store = get_qdrant_vector_store(collection_name)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    return retriever\n",
    "\n",
    "def rerank_documents(query, documents, top_n=3):\n",
    "    \"\"\"\n",
    "    Reranks the retrieved documents using Cohere's rerank API.\n",
    "    \"\"\"\n",
    "    co = cohere.Client(cohere_api_key)\n",
    "    \n",
    "    docs = [doc.page_content for doc in documents]\n",
    "    results = co.rerank(query=query, documents=docs, top_n=top_n, model=\"rerank-english-v2.0\")\n",
    "    \n",
    "    reranked_docs = [documents[result.index] for result in results.results]\n",
    "    return reranked_docs\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# retriever = get_retriever()\n",
    "# retrieved_docs = retriever.get_relevant_documents(\"your query\")\n",
    "# reranked_docs = rerank_documents(\"your query\", retrieved_docs)\n",
    "# for i, doc in enumerate(reranked_docs):\n",
    "#     print(f\"Reranked Doc {i+1}:\")\n",
    "#     print(doc.page_content)\n",
    "#     print(doc.metadata)\n",
    "#     print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM and Answering\n",
    "def get_conversational_chain():\n",
    "    \"\"\"\n",
    "    Initializes and returns a conversational retrieval QA chain.\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "    \n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=model,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=get_retriever(),\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return chain\n",
    "\n",
    "def generate_response(query, chain):\n",
    "    \"\"\"\n",
    "    Generates a response to the user's query using the conversational chain.\n",
    "    \"\"\"\n",
    "    result = chain({\"query\": query})\n",
    "    \n",
    "    answer = result[\"result\"]\n",
    "    source_documents = result[\"source_documents\"]\n",
    "    \n",
    "    # Create citations\n",
    "    citations = []\n",
    "    for i, doc in enumerate(source_documents):\n",
    "        citation = f\"[{i+1}] {doc.metadata.get('title', 'Unknown Title')} (chunk {doc.metadata.get('position', 'N/A')})\"\n",
    "        citations.append(citation)\n",
    "        \n",
    "    # Add citations to the answer\n",
    "    full_response = f\"{answer}\\n\\n**Sources:**\\n\" + \"\\n\".join(citations)\n",
    "    \n",
    "    return full_response\n",
    "\n",
    "# Example usage (we will use this later in the Streamlit app)\n",
    "# chain = get_conversational_chain()\n",
    "# response = generate_response(\"your query\", chain)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a7d812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import os\n",
    "import time\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import qdrant_client\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
    "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
    "\n",
    "# Data Ingestion and Chunking\n",
    "def process_and_chunk_data(text_data, chunk_size=1000, chunk_overlap=100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_text(text_data)\n",
    "    metadatas = [{\"source\": f\"chunk-{i}\", \"title\": \"User Uploaded Data\", \"position\": i} for i in range(len(docs))]\n",
    "    return docs, metadatas\n",
    "\n",
    "# Vector Store (Qdrant)\n",
    "def get_qdrant_vector_store(collection_name=\"user-data-collection\"):\n",
    "    client = qdrant_client.QdrantClient(qdrant_url, api_key=qdrant_api_key)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "    vector_store = Qdrant(client=client, collection_name=collection_name, embeddings=embeddings)\n",
    "    return vector_store\n",
    "\n",
    "def upsert_data_to_qdrant(documents, metadatas, collection_name=\"user-data-collection\"):\n",
    "    vector_store = get_qdrant_vector_store(collection_name)\n",
    "    vector_store.add_texts(documents, metadatas)\n",
    "\n",
    "# Retriever and Reranker\n",
    "def get_retriever(collection_name=\"user-data-collection\", top_k=5):\n",
    "    vector_store = get_qdrant_vector_store(collection_name)\n",
    "    return vector_store.as_retriever(search_kwargs={\"k\": top_k})\n",
    "\n",
    "def rerank_documents(query, documents, top_n=3):\n",
    "    co = cohere.Client(cohere_api_key)\n",
    "    docs = [doc.page_content for doc in documents]\n",
    "    results = co.rerank(query=query, documents=docs, top_n=top_n, model=\"rerank-english-v2.0\")\n",
    "    return [documents[result.index] for result in results.results]\n",
    "\n",
    "# LLM and Answering\n",
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    chain = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=get_retriever(), chain_type_kwargs={\"prompt\": prompt}, return_source_documents=True)\n",
    "    return chain\n",
    "\n",
    "def generate_response(query, chain):\n",
    "    result = chain({\"query\": query})\n",
    "    answer = result[\"result\"]\n",
    "    source_documents = result[\"source_documents\"]\n",
    "    \n",
    "    citations = [f\"[{i+1}] {doc.metadata.get('title', 'Unknown Title')} (chunk {doc.metadata.get('position', 'N/A')})\" for i, doc in enumerate(source_documents)]\n",
    "    full_response = f\"{answer}\\n\\n**Sources:**\\n\" + \"\\n\".join(citations)\n",
    "    \n",
    "    return full_response, source_documents\n",
    "\n",
    "# --- Streamlit App ---\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Chatbot\", layout=\"wide\")\n",
    "st.title(\"RAG Chatbot: Qdrant, Gemini, and Cohere\")\n",
    "\n",
    "# --- Sidebar for Data Ingestion ---\n",
    "with st.sidebar:\n",
    "    st.header(\"Data Ingestion\")\n",
    "    uploaded_file = st.file_uploader(\"Upload a text file\", type=[\"txt\"])\n",
    "    text_input = st.text_area(\"Or paste your text here\", height=200)\n",
    "    \n",
    "    if st.button(\"Process and Store Data\"):\n",
    "        if uploaded_file is not None:\n",
    "            text_data = uploaded_file.read().decode(\"utf-8\")\n",
    "            st.info(\"Processing uploaded file.\")\n",
    "        elif text_input.strip():\n",
    "            text_data = text_input\n",
    "            st.info(\"Processing pasted text.\")\n",
    "        else:\n",
    "            st.warning(\"Please upload a file or paste text to process.\")\n",
    "            st.stop()\n",
    "\n",
    "        with st.spinner(\"Chunking and upserting data to Qdrant...\"):\n",
    "            start_time = time.time()\n",
    "            documents, metadatas = process_and_chunk_data(text_data)\n",
    "            upsert_data_to_qdrant(documents, metadatas)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            st.success(f\"Successfully processed and stored {len(documents)} chunks in {end_time - start_time:.2f} seconds.\")\n",
    "            st.session_state.data_processed = True\n",
    "\n",
    "# --- Main Chat Interface ---\n",
    "st.header(\"Ask Questions\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "if \"data_processed\" not in st.session_state:\n",
    "    st.session_state.data_processed = False\n",
    "\n",
    "if not st.session_state.data_processed:\n",
    "    st.warning(\"Please process some data first using the sidebar.\")\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "if query := st.chat_input(\"What is your question?\"):\n",
    "    if not st.session_state.data_processed:\n",
    "        st.warning(\"Please process data before asking questions.\")\n",
    "        st.stop()\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(query)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Retrieval\n",
    "            retriever = get_retriever()\n",
    "            retrieved_docs = retriever.get_relevant_documents(query)\n",
    "            \n",
    "            # Reranking\n",
    "            reranked_docs = rerank_documents(query, retrieved_docs)\n",
    "            \n",
    "            # Answering\n",
    "            chain = get_conversational_chain()\n",
    "            # We need to pass the reranked docs as context to the chain\n",
    "            # The current chain setup doesn't directly support this.\n",
    "            # Let's modify how we generate the response.\n",
    "            \n",
    "            context = \"\\n\\n\".join([doc.page_content for doc in reranked_docs])\n",
    "            prompt_template = \"\"\"\n",
    "            Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "            provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "            Context:\\n {context}?\\n\n",
    "            Question: \\n{question}\\n\n",
    "\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
    "            prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "            \n",
    "            from langchain.chains.llm import LLMChain\n",
    "            llm_chain = LLMChain(llm=model, prompt=prompt)\n",
    "            answer = llm_chain.run({\"context\": context, \"question\": query})\n",
    "\n",
    "            # Create citations from reranked docs\n",
    "            citations = [f\"[{i+1}] {doc.metadata.get('title', 'Unknown Title')} (chunk {doc.metadata.get('position', 'N/A')})\" for i, doc in enumerate(reranked_docs)]\n",
    "            full_response = f\"{answer}\\n\\n**Sources:**\\n\" + \"\\n\".join(citations)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            st.markdown(full_response)\n",
    "            \n",
    "            # Display timing and cost (rough estimate)\n",
    "            st.info(f\"Response generated in {end_time - start_time:.2f} seconds.\")\n",
    "            # Rough token count and cost - replace with actuals if needed\n",
    "            # This is a very rough estimation\n",
    "            input_tokens = len(query.split()) + len(context.split())\n",
    "            output_tokens = len(answer.split())\n",
    "            st.info(f\"Estimated tokens: {input_tokens + output_tokens}\")\n",
    "\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "\n",
    "    with st.expander(\"Show Reranked Sources\"):\n",
    "        for i, doc in enumerate(reranked_docs):\n",
    "            st.write(f\"**Source [{i+1}]**\")\n",
    "            st.write(f\"**Title:** {doc.metadata.get('title', 'N/A')}\")\n",
    "            st.write(f\"**Chunk Position:** {doc.metadata.get('position', 'N/A')}\")\n",
    "            st.write(doc.page_content)\n",
    "            st.divider()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
